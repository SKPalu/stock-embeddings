{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No change made because period entered is 1\n",
      "16 627 152.75 458.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [00:07<00:00, 85.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 627 152.75 458.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [00:01<00:00, 477.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 627 152.75 458.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611/611 [00:00<00:00, 1556.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.returns_data_class import ReturnsData\n",
    "from utils.pointwise_context import get_contextual_indices\n",
    "\n",
    "# Constants and configurations\n",
    "PERIODS = [1, 5, 10]\n",
    "TRAIN_PCT = 1\n",
    "LOAD_IDX_COMBINATIONS = False\n",
    "EPOCHS = 3\n",
    "EMBEDDING_DIM = 20\n",
    "CONTEXT_SIZE = 32\n",
    "SAVE_MODEL = False\n",
    "SAVE_PATH_TEMPLATE = \"embeddings/abs_diff_E{epochs}_C{context_size}_D{embedding_dim}_P{periods}_train{train_pct}.pt\"\n",
    "save_path = SAVE_PATH_TEMPLATE.format(\n",
    "    epochs=EPOCHS,\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    periods=\"-\".join(map(str, PERIODS)),\n",
    "    train_pct=TRAIN_PCT,\n",
    ")\n",
    "\n",
    "idx_combinations = []\n",
    "for period in PERIODS:\n",
    "    data = ReturnsData(\n",
    "        daily_returns_path=\"Data/returns_df_611.csv\",\n",
    "        extras_path=\"Data/historical_stocks.csv\",\n",
    "    )\n",
    "    data.change_returns_period(period)\n",
    "    data.train_test_split(TRAIN_PCT)\n",
    "\n",
    "    idx_combinations += get_contextual_indices(\n",
    "        data.train_returns_df,\n",
    "        context_size=CONTEXT_SIZE,\n",
    "        verbose=True,\n",
    "        iqr_noise_reduction=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'BAC', 'STI', 'WFC', 'PNC', 'HBAN', 'ZION', 'STT', 'KEY', 'CMA']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = data.ticker2idx[\"JPM\"]\n",
    "import numpy as np\n",
    "\n",
    "temp = np.array([xi[1] for xi in idx_combinations if xi[0] == i]).flatten()\n",
    "import pandas as pd\n",
    "\n",
    "[data.idx2ticker[xi] for xi in pd.Series(temp).value_counts().index][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.base_model import BaseModel\n",
    "\n",
    "\n",
    "class ClassificationEmbeddings(BaseModel):\n",
    "    \"\"\"\n",
    "    Model architecture similar to CBOW Word2Vec but adapted for stock modelling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_time_series: int, embedding_dim: int):\n",
    "        super(ClassificationEmbeddings, self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_time_series, embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # -- This extracts the relevant rows of the embedding matrix\n",
    "        # - Equivalent to W^T x_i in \"word2vec Parameter Learning Explained\"\n",
    "        temp = self.embeddings(inputs)  # .view((len(inputs),-1))\n",
    "\n",
    "        # -- Compute the hidden layer by a simple mean\n",
    "        hidden = temp.mean(axis=1)\n",
    "        # -- Reshape to make matrix dimensions compatible\n",
    "        hidden = hidden.unsqueeze(dim=2)\n",
    "        # -- Compute dot product of hidden with embeddings\n",
    "        # out = torch.einsum(\"nd,\", self.embeddings.weight, hidden)\n",
    "        out = torch.matmul(self.embeddings.weight, hidden)\n",
    "\n",
    "        # -- Return the log softmax since we use NLLLoss loss function\n",
    "        return nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationEmbeddings(\n",
    "    n_time_series=len(data.tickers), embedding_dim=EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:40<12:58, 40.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.10374109499065473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:21<12:14, 40.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.10121887932745742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:02<11:32, 40.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.0995641483254468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [02:43<10:51, 40.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.09868489761125318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:23<10:11, 40.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.09819312226299613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [04:05<09:32, 40.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.09788414273028445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [04:46<08:52, 40.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.0976704512347089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [05:27<10:07, 46.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.09751443594345513\n",
      "Early stopping at epoch 7 due to minimal loss reduction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.training_helpers import train_embeddings_from_idx_combinations\n",
    "\n",
    "model, losses = train_embeddings_from_idx_combinations(\n",
    "    n_time_series=len(data.tickers),\n",
    "    idx_combinations=idx_combinations,\n",
    "    model=model,\n",
    "    epochs=20,\n",
    "    # embedding_dim=EMBEDDING_DIM,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.6\n",
      "Recall Score: 0.58\n",
      "F1 Score: 0.59\n",
      "Accuracy Score: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/phd/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from utils.sector_classification import get_sector_score\n",
    "\n",
    "get_sector_score(model.embeddings.weight.detach().numpy(), sectors=data.sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
